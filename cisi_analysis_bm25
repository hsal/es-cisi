import os
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from rank_bm25 import BM25Okapi

# Ensure necessary NLTK resources are available
nltk.download('stopwords')
nltk.download('punkt')

# Load the dataset (Modify the path to your dataset location)
DOCUMENTS_FILE = "CISI.ALL"
QUERIES_FILE = "CISI.QRY"
RELEVANCE_FILE = "CISI.REL"

# Function to read CISI dataset files
def read_cisi_file(filename):
    """Reads a CISI dataset file and returns a dictionary mapping IDs to text."""
    data = {}
    with open(filename, "r", encoding="utf-8") as file:
        content = file.read()
    
    # CISI documents have an identifier like .I 123
    entries = re.split(r'\.I \d+', content)[1:]
    for i, entry in enumerate(entries):
        entry = entry.strip()
        data[i + 1] = entry  # Assign an index-based ID
    return data

# Load CISI documents and queries
documents = read_cisi_file(DOCUMENTS_FILE)
queries = read_cisi_file(QUERIES_FILE)

# Load Relevance Judgments
def load_relevance_judgments(filename):
    """Reads the relevance judgments file and returns a dictionary mapping queries to relevant documents."""
    relevance = {}
    with open(filename, "r", encoding="utf-8") as file:
        for line in file:
            parts = line.strip().split()
            if len(parts) == 2:
                query_id, doc_id = map(int, parts)
                if query_id not in relevance:
                    relevance[query_id] = set()
                relevance[query_id].add(doc_id)
    return relevance

relevance_judgments = load_relevance_judgments(RELEVANCE_FILE)

# Convert to DataFrames
doc_df = pd.DataFrame(list(documents.items()), columns=["DocID", "Text"])
query_df = pd.DataFrame(list(queries.items()), columns=["QueryID", "Text"])

# Text Preprocessing
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Remove special characters
    words = nltk.word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if w not in stop_words]
    stemmer = PorterStemmer()
    words = [stemmer.stem(w) for w in words]
    return words  # Return list of words for BM25

# Apply preprocessing
doc_df["ProcessedText"] = doc_df["Text"].apply(preprocess_text)
query_df["ProcessedText"] = query_df["Text"].apply(preprocess_text)

# BM25 Retrieval Model
bm25 = BM25Okapi(doc_df["ProcessedText"].tolist())

# Retrieve Top Documents for Each Query using BM25
num_results = 5
retrieval_results = {}
for i, query_id in enumerate(query_df["QueryID"]):
    scores = bm25.get_scores(query_df.iloc[i]["ProcessedText"])
    sorted_indices = np.argsort(-scores)  # Sort by descending score
    top_docs = [(doc_df.iloc[idx]["DocID"], scores[idx]) for idx in sorted_indices[:num_results]]
    retrieval_results[query_id] = top_docs

# Compute Evaluation Metrics
def precision_at_k(retrieved_docs, relevant_docs, k):
    retrieved_k = retrieved_docs[:k]
    relevant_count = sum(1 for doc in retrieved_k if doc in relevant_docs)
    return relevant_count / k if k else 0

def recall_at_k(retrieved_docs, relevant_docs, k):
    relevant_count = sum(1 for doc in retrieved_docs[:k] if doc in relevant_docs)
    return relevant_count / len(relevant_docs) if relevant_docs else 0

def mean_average_precision(retrieval_results, relevance_judgments):
    average_precisions = []
    for query_id, retrieved_docs in retrieval_results.items():
        relevant_docs = relevance_judgments.get(query_id, set())
        if not relevant_docs:
            continue
        precisions = [precision_at_k([doc for doc, _ in retrieved_docs], relevant_docs, k + 1)
                      for k in range(len(retrieved_docs)) if retrieved_docs[k][0] in relevant_docs]
        if precisions:
            average_precisions.append(sum(precisions) / len(relevant_docs))
    return np.mean(average_precisions) if average_precisions else 0

def mean_reciprocal_rank(retrieval_results, relevance_judgments):
    reciprocals = []
    for query_id, retrieved_docs in retrieval_results.items():
        relevant_docs = relevance_judgments.get(query_id, set())
        for rank, (doc_id, _) in enumerate(retrieved_docs, start=1):
            if doc_id in relevant_docs:
                reciprocals.append(1 / rank)
                break
    return np.mean(reciprocals) if reciprocals else 0

map_score = mean_average_precision(retrieval_results, relevance_judgments)
mrr_score = mean_reciprocal_rank(retrieval_results, relevance_judgments)

print(f"Mean Average Precision (MAP): {map_score:.4f}")
print(f"Mean Reciprocal Rank (MRR): {mrr_score:.4f}")

# Display Sample Results
for query_id, results in list(retrieval_results.items())[:3]:  # Show first 3 queries
    print(f"Query {query_id}:")
    for doc_id, score in results:
        print(f"   Document {doc_id} - BM25 Score: {score:.4f}")
    print("\n")